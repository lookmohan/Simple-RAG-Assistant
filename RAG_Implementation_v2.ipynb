{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TvrmsKCYQpI-TFqv5zpiMmUCyv7bUibM",
      "authorship_tag": "ABX9TyPN/6K0KEvKuWi8Z6ostFdd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lookmohan/Simple-RAG-Assistant/blob/main/RAG_Implementation_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Required Libraries"
      ],
      "metadata": {
        "id": "WhBHmPV0Z6Zk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZED6xVFmZu-s"
      },
      "outputs": [],
      "source": [
        "!pip install -q chromadb sentence-transformers langchain langchain-groq langchain-community langchain-google-genai pypdf docx2txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Setup"
      ],
      "metadata": {
        "id": "tUDTz1lyaKB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chromadb\n",
        "import json\n",
        "import pickle\n",
        "from typing import List, Dict, Any\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from google.colab import files\n",
        "import glob\n",
        "import sys\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "tqNZRI2KaFqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GROQ_API_KEY = userdata.get('grok_api_key')\n",
        "\n",
        "# Storage configuration\n",
        "STORAGE_DIR = \"./rag_storage\"\n",
        "CHROMA_DB_PATH = \"./chroma_db\"\n",
        "CONVERSATION_HISTORY_FILE = f\"{STORAGE_DIR}/conversation_history.json\"\n",
        "METADATA_FILE = f\"{STORAGE_DIR}/metadata.json\"\n",
        "\n",
        "# Create storage directories\n",
        "os.makedirs(STORAGE_DIR, exist_ok=True)\n",
        "os.makedirs(CHROMA_DB_PATH, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Configuration loaded!\")\n",
        "print(f\"üìÅ Storage directory: {STORAGE_DIR}\")\n",
        "print(f\"üìÅ Vector DB directory: {CHROMA_DB_PATH}\")"
      ],
      "metadata": {
        "id": "V7OBAh5paahb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_print(text):\n",
        "    \"\"\"Safe printing function to handle Unicode errors.\"\"\"\n",
        "    try:\n",
        "        print(text)\n",
        "    except UnicodeEncodeError:\n",
        "        encoding = sys.stdout.encoding or 'ascii'\n",
        "        encoded_text = text.encode(encoding, errors='replace')\n",
        "        print(encoded_text.decode(encoding, errors='replace'))\n",
        "    except Exception as e:\n",
        "        print(f\"[Error in safe_print]: Could not print message due to: {e}\")\n",
        "\n",
        "\n",
        "def save_json(data: Any, filepath: str):\n",
        "    \"\"\"Save data to JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        safe_print(f\"‚ùå Error saving to {filepath}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def load_json(filepath: str) -> Any:\n",
        "    \"\"\"Load data from JSON file.\"\"\"\n",
        "    try:\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        safe_print(f\"‚ùå Error loading from {filepath}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "3H9Px8gUa6vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Database Class with Persistence"
      ],
      "metadata": {
        "id": "vbjFll7Ua_gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorDB:\n",
        "    \"\"\"Vector database with persistent storage using ChromaDB.\"\"\"\n",
        "\n",
        "    def __init__(self, collection_name: str = \"rag_documents\",\n",
        "                 embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                 persist_directory: str = CHROMA_DB_PATH):\n",
        "        \"\"\"Initialize the vector database with persistence.\"\"\"\n",
        "        self.collection_name = collection_name\n",
        "        self.embedding_model_name = embedding_model\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "        self.persist_directory = persist_directory\n",
        "\n",
        "        # Initialize ChromaDB client with persistence\n",
        "        self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
        "\n",
        "        # Get or create collection\n",
        "        self.collection = self.client.get_or_create_collection(\n",
        "            name=self.collection_name,\n",
        "            metadata={\"description\": \"RAG document collection\"}\n",
        "        )\n",
        "\n",
        "        # Check if collection has existing data\n",
        "        existing_count = self.collection.count()\n",
        "        safe_print(f\"‚úÖ Vector database initialized: {self.collection_name}\")\n",
        "        if existing_count > 0:\n",
        "            safe_print(f\"üìö Found {existing_count} existing document chunks in storage\")\n",
        "\n",
        "    def chunk_text(self, text: str, chunk_size=2000, chunk_overlap=200) -> List[str]:\n",
        "        \"\"\"Split text into chunks.\"\"\"\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=['\\n\\n', '\\n', ' ', '']\n",
        "        )\n",
        "        chunks = text_splitter.split_text(text)\n",
        "        return chunks\n",
        "\n",
        "    def add_documents(self, documents: List[Dict[str, Any]]) -> None:\n",
        "        \"\"\"Add documents to the vector database with persistence.\"\"\"\n",
        "        safe_print(f\"üìÑ Processing {len(documents)} documents...\")\n",
        "\n",
        "        all_chunks = []\n",
        "        all_metadatas = []\n",
        "        all_ids = []\n",
        "\n",
        "        for doc_idx, doc in enumerate(documents):\n",
        "            content = doc.get('content', '')\n",
        "            metadata = doc.get('metadata', {})\n",
        "\n",
        "            # Chunk the document\n",
        "            chunks = self.chunk_text(content)\n",
        "\n",
        "            # Create metadata and IDs for each chunk\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                all_chunks.append(chunk)\n",
        "\n",
        "                chunk_metadata = metadata.copy()\n",
        "                chunk_metadata['chunk_index'] = chunk_idx\n",
        "                chunk_metadata['doc_index'] = doc_idx\n",
        "                chunk_metadata['added_date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                all_metadatas.append(chunk_metadata)\n",
        "\n",
        "                # Create UNIQUE id with timestamp to avoid conflicts\n",
        "                timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")\n",
        "                all_ids.append(f'doc_{doc_idx}_chunk_{chunk_idx}_{timestamp}')\n",
        "\n",
        "        if all_chunks:\n",
        "            self.collection.add(\n",
        "                documents=all_chunks,\n",
        "                metadatas=all_metadatas,\n",
        "                ids=all_ids\n",
        "            )\n",
        "            safe_print(f\"‚úÖ Added {len(all_chunks)} chunks to persistent vector database\")\n",
        "        else:\n",
        "            safe_print(\"‚ö†Ô∏è No chunks to add\")\n",
        "\n",
        "    def search(self, query: str, n_results: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"Search for relevant documents.\"\"\"\n",
        "        query_embedding = self.embedding_model.encode([query]).tolist()\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=query_embedding,\n",
        "            n_results=n_results\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get statistics about the vector database.\"\"\"\n",
        "        count = self.collection.count()\n",
        "        return {\n",
        "            \"total_chunks\": count,\n",
        "            \"collection_name\": self.collection_name,\n",
        "            \"persist_directory\": self.persist_directory\n",
        "        }\n",
        "\n",
        "    def clear_database(self):\n",
        "        \"\"\"Clear all documents from the database.\"\"\"\n",
        "        try:\n",
        "            self.client.delete_collection(name=self.collection_name)\n",
        "            self.collection = self.client.get_or_create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"RAG document collection\"}\n",
        "            )\n",
        "            safe_print(\"‚úÖ Vector database cleared!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            safe_print(f\"‚ùå Error clearing database: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "print(\"‚úÖ VectorDB class defined with persistence!\")\n"
      ],
      "metadata": {
        "id": "c_KtM-xva8NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Loading Functions"
      ],
      "metadata": {
        "id": "mQiZh9KubNOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents_from_folder(folder_path: str = './data') -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load documents from a folder (supports .txt, .pdf, .docx).\"\"\"\n",
        "    documents = []\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        safe_print(f\"üìÅ Creating folder: {folder_path}\")\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "        return documents\n",
        "\n",
        "    # Load .txt files\n",
        "    for file_path in glob.glob(f\"{folder_path}/*.txt\"):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                content = f.read()\n",
        "                documents.append({\n",
        "                    'content': content,\n",
        "                    'metadata': {\n",
        "                        'source': os.path.basename(file_path),\n",
        "                        'type': 'txt',\n",
        "                        'full_path': file_path\n",
        "                    }\n",
        "                })\n",
        "                safe_print(f\"‚úÖ Loaded: {os.path.basename(file_path)}\")\n",
        "        except Exception as e:\n",
        "            safe_print(f\"‚ùå Error loading {file_path}: {e}\")\n",
        "\n",
        "    # Load .pdf files\n",
        "    try:\n",
        "        from langchain_community.document_loaders import PyPDFLoader\n",
        "        for file_path in glob.glob(f\"{folder_path}/*.pdf\"):\n",
        "            try:\n",
        "                loader = PyPDFLoader(file_path)\n",
        "                pages = loader.load()\n",
        "                content = \"\\n\\n\".join([page.page_content for page in pages])\n",
        "                documents.append({\n",
        "                    'content': content,\n",
        "                    'metadata': {\n",
        "                        'source': os.path.basename(file_path),\n",
        "                        'type': 'pdf',\n",
        "                        'full_path': file_path,\n",
        "                        'pages': len(pages)\n",
        "                    }\n",
        "                })\n",
        "                safe_print(f\"‚úÖ Loaded: {os.path.basename(file_path)} ({len(pages)} pages)\")\n",
        "            except Exception as e:\n",
        "                safe_print(f\"‚ùå Error loading {file_path}: {e}\")\n",
        "    except ImportError:\n",
        "        safe_print(\"‚ö†Ô∏è PyPDFLoader not available, skipping PDF files\")\n",
        "\n",
        "    # Load .docx files\n",
        "    try:\n",
        "        import docx2txt\n",
        "        for file_path in glob.glob(f\"{folder_path}/*.docx\"):\n",
        "            try:\n",
        "                content = docx2txt.process(file_path)\n",
        "                documents.append({\n",
        "                    'content': content,\n",
        "                    'metadata': {\n",
        "                        'source': os.path.basename(file_path),\n",
        "                        'type': 'docx',\n",
        "                        'full_path': file_path\n",
        "                    }\n",
        "                })\n",
        "                safe_print(f\"‚úÖ Loaded: {os.path.basename(file_path)}\")\n",
        "            except Exception as e:\n",
        "                safe_print(f\"‚ùå Error loading {file_path}: {e}\")\n",
        "    except ImportError:\n",
        "        safe_print(\"‚ö†Ô∏è docx2txt not available, skipping DOCX files\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n",
        "print(\"‚úÖ Document loading functions defined!\")\n"
      ],
      "metadata": {
        "id": "2qxpIgP4bPAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Memory Class with Persistence"
      ],
      "metadata": {
        "id": "UaSfkz0xbZ-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationMemory:\n",
        "    \"\"\"Manages conversation history with persistent storage.\"\"\"\n",
        "\n",
        "    def __init__(self, max_history: int = 50, storage_file: str = CONVERSATION_HISTORY_FILE):\n",
        "        \"\"\"Initialize conversation memory with persistence.\"\"\"\n",
        "        self.max_history = max_history\n",
        "        self.storage_file = storage_file\n",
        "        self.history = []\n",
        "        self.session_start = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        # Load existing history if available\n",
        "        self.load_history()\n",
        "\n",
        "    def add_exchange(self, question: str, answer: str):\n",
        "        \"\"\"Add a Q&A exchange to history and save to storage.\"\"\"\n",
        "        exchange = {\n",
        "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'question': question,\n",
        "            'answer': answer\n",
        "        }\n",
        "        self.history.append(exchange)\n",
        "\n",
        "        # Keep only the last max_history exchanges\n",
        "        if len(self.history) > self.max_history:\n",
        "            self.history = self.history[-self.max_history:]\n",
        "\n",
        "        # Save to persistent storage\n",
        "        self.save_history()\n",
        "\n",
        "    def get_formatted_history(self, num_exchanges: int = 5) -> str:\n",
        "        \"\"\"Get formatted conversation history.\"\"\"\n",
        "        if not self.history:\n",
        "            return \"No previous conversation.\"\n",
        "\n",
        "        recent_history = self.history[-num_exchanges:]\n",
        "\n",
        "        formatted = \"CONVERSATION HISTORY:\\n\" + \"=\" * 50 + \"\\n\"\n",
        "        for i, exchange in enumerate(recent_history, 1):\n",
        "            formatted += f\"\\nExchange {i} ({exchange['timestamp']}):\\n\"\n",
        "            formatted += f\"User: {exchange['question']}\\n\"\n",
        "            formatted += f\"Assistant: {exchange['answer'][:200]}{'...' if len(exchange['answer']) > 200 else ''}\\n\"\n",
        "            formatted += \"-\" * 50 + \"\\n\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def get_all_questions(self) -> List[str]:\n",
        "        \"\"\"Get all questions from history.\"\"\"\n",
        "        return [exchange['question'] for exchange in self.history]\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"Save conversation history to persistent storage.\"\"\"\n",
        "        data = {\n",
        "            'session_start': self.session_start,\n",
        "            'last_updated': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'total_exchanges': len(self.history),\n",
        "            'history': self.history\n",
        "        }\n",
        "        save_json(data, self.storage_file)\n",
        "\n",
        "    def load_history(self):\n",
        "        \"\"\"Load conversation history from persistent storage.\"\"\"\n",
        "        data = load_json(self.storage_file)\n",
        "        if data and 'history' in data:\n",
        "            self.history = data['history']\n",
        "            safe_print(f\"üìú Loaded {len(self.history)} previous conversation exchanges\")\n",
        "            if 'session_start' in data:\n",
        "                safe_print(f\"üìÖ Previous session started: {data['session_start']}\")\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clear conversation history.\"\"\"\n",
        "        self.history = []\n",
        "        self.session_start = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        self.save_history()\n",
        "\n",
        "    def export_history(self, filepath: str = None):\n",
        "        \"\"\"Export conversation history to a text file.\"\"\"\n",
        "        if not filepath:\n",
        "            filepath = f\"{STORAGE_DIR}/conversation_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "\n",
        "        try:\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"RAG ASSISTANT - CONVERSATION HISTORY\\n\")\n",
        "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "                f.write(f\"Session Start: {self.session_start}\\n\")\n",
        "                f.write(f\"Total Exchanges: {len(self.history)}\\n\")\n",
        "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "\n",
        "                for i, exchange in enumerate(self.history, 1):\n",
        "                    f.write(f\"Exchange {i} - {exchange['timestamp']}\\n\")\n",
        "                    f.write(f\"User: {exchange['question']}\\n\\n\")\n",
        "                    f.write(f\"Assistant: {exchange['answer']}\\n\\n\")\n",
        "                    f.write(\"-\" * 60 + \"\\n\\n\")\n",
        "\n",
        "            safe_print(f\"‚úÖ Conversation history exported to: {filepath}\")\n",
        "            return filepath\n",
        "        except Exception as e:\n",
        "            safe_print(f\"‚ùå Error exporting history: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "print(\"‚úÖ ConversationMemory class defined with persistence!\")\n"
      ],
      "metadata": {
        "id": "tcv743UXbbtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Assistant Class with Full Persistence"
      ],
      "metadata": {
        "id": "JxW2BYqHbi5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGAssistant:\n",
        "    \"\"\"Advanced RAG-based AI assistant with full persistence.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        \"\"\"Initialize the RAG assistant with Groq and persistence.\"\"\"\n",
        "        self.llm = ChatGroq(\n",
        "            groq_api_key=api_key,\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        self.vector_db = VectorDB()\n",
        "        self.memory = ConversationMemory(max_history=50)\n",
        "        self.metadata = self._load_metadata()\n",
        "\n",
        "        template = \"\"\"You are an advanced AI assistant that provides accurate, helpful answers based on provided documents and conversation history.\n",
        "\n",
        "CORE PRINCIPLES:\n",
        "- Answer based on the provided documents AND conversation history\n",
        "- Remember previous questions and answers in this conversation\n",
        "- Be clear, concise, and accurate\n",
        "- Cite sources when relevant\n",
        "- If information isn't in the documents, say so honestly\n",
        "- If asked about previous conversation, refer to the conversation history\n",
        "\n",
        "RESPONSE GUIDELINES:\n",
        "- For definitions/concepts: Use clear explanations with examples\n",
        "- For comparisons: Use structured format\n",
        "- For procedures: Provide step-by-step instructions\n",
        "- For questions about previous conversation: Use the conversation history\n",
        "- For general questions: Keep answers concise and well-organized\n",
        "\n",
        "{conversation_history}\n",
        "\n",
        "CONTEXT FROM DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "CURRENT USER QUESTION:\n",
        "{question}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "\n",
        "        self.prompt_template = ChatPromptTemplate.from_template(template)\n",
        "        self.chain = self.prompt_template | self.llm | StrOutputParser()\n",
        "\n",
        "        safe_print(\"‚úÖ RAG Assistant initialized with Groq (Llama 3.3 70B)\")\n",
        "        safe_print(\"‚úÖ Conversation memory enabled with persistence\")\n",
        "        safe_print(\"‚úÖ Vector database persistent storage active\")\n",
        "\n",
        "    def _load_metadata(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load or create system metadata.\"\"\"\n",
        "        metadata = load_json(METADATA_FILE)\n",
        "        if not metadata:\n",
        "            metadata = {\n",
        "                'created_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'total_documents_processed': 0,\n",
        "                'total_queries': 0,\n",
        "                'document_list': []\n",
        "            }\n",
        "            save_json(metadata, METADATA_FILE)\n",
        "        return metadata\n",
        "\n",
        "    def _update_metadata(self, **kwargs):\n",
        "        \"\"\"Update system metadata.\"\"\"\n",
        "        self.metadata.update(kwargs)\n",
        "        self.metadata['last_updated'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        save_json(self.metadata, METADATA_FILE)\n",
        "\n",
        "    def add_documents(self, documents: List[Dict[str, Any]]) -> None:\n",
        "        \"\"\"Add documents to the knowledge base with metadata tracking.\"\"\"\n",
        "        self.vector_db.add_documents(documents)\n",
        "\n",
        "        doc_names = [doc['metadata']['source'] for doc in documents]\n",
        "        self.metadata['total_documents_processed'] += len(documents)\n",
        "        self.metadata['document_list'].extend(doc_names)\n",
        "        self._update_metadata()\n",
        "\n",
        "    def query(self, question: str, n_results: int = 3) -> str:\n",
        "        \"\"\"Query the RAG assistant with conversation context.\"\"\"\n",
        "        self.metadata['total_queries'] += 1\n",
        "        self._update_metadata()\n",
        "\n",
        "        history_keywords = ['first question', 'previous', 'earlier', 'before', 'what did i ask',\n",
        "                           'conversation', 'history', 'last question', 'my question']\n",
        "        is_history_question = any(keyword in question.lower() for keyword in history_keywords)\n",
        "\n",
        "        search_results = self.vector_db.search(question, n_results=n_results)\n",
        "        context_chunks = search_results.get('documents', [[]])[0]\n",
        "        metadatas = search_results.get('metadatas', [[]])[0]\n",
        "\n",
        "        formatted_context = \"\"\n",
        "        sources = []\n",
        "\n",
        "        for i, (chunk, meta) in enumerate(zip(context_chunks, metadatas)):\n",
        "            source = meta.get('source', 'Unknown')\n",
        "            sources.append(source)\n",
        "            formatted_context += f\"\\n[Source: {source}]\\n{chunk}\\n{'-' * 40}\\n\"\n",
        "\n",
        "        conversation_history = self.memory.get_formatted_history(num_exchanges=5)\n",
        "\n",
        "        answer = self.chain.invoke({\n",
        "            \"conversation_history\": conversation_history,\n",
        "            \"context\": formatted_context,\n",
        "            \"question\": question\n",
        "        })\n",
        "\n",
        "        unique_sources = list(set(sources))\n",
        "        if unique_sources and \"source\" not in answer.lower() and not is_history_question:\n",
        "            answer += f\"\\n\\nüìö Sources: {', '.join(unique_sources)}\"\n",
        "\n",
        "        self.memory.add_exchange(question, answer)\n",
        "        return answer\n",
        "\n",
        "    def get_conversation_summary(self) -> str:\n",
        "        \"\"\"Get a summary of the conversation.\"\"\"\n",
        "        if not self.memory.history:\n",
        "            return \"No conversation history yet.\"\n",
        "\n",
        "        all_questions = self.memory.get_all_questions()\n",
        "        summary = f\"üìä CONVERSATION SUMMARY\\n{'=' * 50}\\n\"\n",
        "        summary += f\"Total questions: {len(all_questions)}\\n\"\n",
        "        summary += f\"Session started: {self.memory.session_start}\\n{'=' * 50}\\n\\n\"\n",
        "        summary += \"Questions:\\n\" + \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(all_questions)])\n",
        "        return summary\n",
        "\n",
        "    def get_system_stats(self) -> str:\n",
        "        \"\"\"Get system statistics.\"\"\"\n",
        "        db_stats = self.vector_db.get_stats()\n",
        "        stats = f\"üìä SYSTEM STATISTICS\\n{'=' * 50}\\n\"\n",
        "        stats += f\"Documents processed: {self.metadata['total_documents_processed']}\\n\"\n",
        "        stats += f\"Queries answered: {self.metadata['total_queries']}\\n\"\n",
        "        stats += f\"Vector DB chunks: {db_stats['total_chunks']}\\n\"\n",
        "        stats += f\"Conversation exchanges: {len(self.memory.history)}\\n\"\n",
        "        stats += f\"System created: {self.metadata['created_date']}\\n\"\n",
        "        stats += f\"Last updated: {self.metadata.get('last_updated', 'N/A')}\\n{'=' * 50}\\n\"\n",
        "        return stats\n",
        "\n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear conversation history.\"\"\"\n",
        "        self.memory.clear()\n",
        "        safe_print(\"üóëÔ∏è Conversation history cleared!\")\n",
        "\n",
        "    def export_conversation(self):\n",
        "        \"\"\"Export conversation history.\"\"\"\n",
        "        return self.memory.export_history()\n",
        "\n",
        "    def reset_system(self):\n",
        "        \"\"\"Reset the entire system.\"\"\"\n",
        "        confirm = input(\"‚ö†Ô∏è This will delete ALL data. Type 'YES' to confirm: \")\n",
        "        if confirm == \"YES\":\n",
        "            self.vector_db.clear_database()\n",
        "            self.memory.clear()\n",
        "            self.metadata = {\n",
        "                'created_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'total_documents_processed': 0,\n",
        "                'total_queries': 0,\n",
        "                'document_list': []\n",
        "            }\n",
        "            self._update_metadata()\n",
        "            safe_print(\"‚úÖ System reset complete!\")\n",
        "        else:\n",
        "            safe_print(\"‚ùå Reset cancelled\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ RAGAssistant class fully defined!\")\n"
      ],
      "metadata": {
        "id": "uxbcmKFPbkvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Documents"
      ],
      "metadata": {
        "id": "F_YWm_wvbyU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "safe_print(\"\\n\" + \"=\" * 60)\n",
        "safe_print(\"üì§ DOCUMENT UPLOAD\")\n",
        "safe_print(\"=\" * 60)\n",
        "safe_print(\"Upload your documents (txt, pdf, docx)\\n\")\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    for filename in uploaded.keys():\n",
        "        with open(f'data/{filename}', 'wb') as f:\n",
        "            f.write(uploaded[filename])\n",
        "    safe_print(f\"\\n‚úÖ Uploaded {len(uploaded)} file(s)\")\n",
        "else:\n",
        "    safe_print(\"\\n‚ö†Ô∏è No files uploaded\")"
      ],
      "metadata": {
        "id": "mZJcqgIfb2A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize RAG System"
      ],
      "metadata": {
        "id": "nNzKH92QcCOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "safe_print(\"\\n\" + \"=\" * 60)\n",
        "safe_print(\"ü§ñ INITIALIZING RAG SYSTEM\")\n",
        "safe_print(\"=\" * 60)\n",
        "\n",
        "if GROQ_API_KEY == \"your_groq_api_key_here\":\n",
        "    safe_print(\"\\n‚ùå ERROR: Configure your Groq API key in CELL 3!\")\n",
        "    safe_print(\"Get free API key: https://console.groq.com/\")\n",
        "else:\n",
        "    assistant = RAGAssistant(api_key=GROQ_API_KEY)\n",
        "\n",
        "    documents = load_documents_from_folder(\"./data\")\n",
        "\n",
        "    if documents:\n",
        "        db_stats = assistant.vector_db.get_stats()\n",
        "        if db_stats['total_chunks'] > 0:\n",
        "            safe_print(f\"üì¶ Database has {db_stats['total_chunks']} chunks\")\n",
        "            choice = input(\"Add new documents? (yes/no): \")\n",
        "            if choice.lower() in ['yes', 'y']:\n",
        "                assistant.add_documents(documents)\n",
        "        else:\n",
        "            assistant.add_documents(documents)\n",
        "\n",
        "        safe_print(\"\\n‚úÖ System Ready!\")\n",
        "        safe_print(assistant.get_system_stats())\n",
        "    else:\n",
        "        safe_print(\"\\n‚ö†Ô∏è No documents found\")"
      ],
      "metadata": {
        "id": "97UAiZUxcEcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Q&A Session"
      ],
      "metadata": {
        "id": "7xeaN7KTcRBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "safe_print(\"\\n\" + \"=\" * 60)\n",
        "safe_print(\"üí¨ INTERACTIVE Q&A SESSION\")\n",
        "safe_print(\"=\" * 60)\n",
        "safe_print(\"\\nüìã Commands:\")\n",
        "safe_print(\"  ‚Ä¢ Ask any question\")\n",
        "safe_print(\"  ‚Ä¢ 'history' - View conversation\")\n",
        "safe_print(\"  ‚Ä¢ 'stats' - System statistics\")\n",
        "safe_print(\"  ‚Ä¢ 'export' - Export conversation\")\n",
        "safe_print(\"  ‚Ä¢ 'clear' - Clear history\")\n",
        "safe_print(\"  ‚Ä¢ 'reset' - Reset system\")\n",
        "safe_print(\"  ‚Ä¢ 'quit' - Exit\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        question = input(\"You: \").strip()\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        if question.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            safe_print(f\"\\nüëã Goodbye! Data saved to: {STORAGE_DIR}\")\n",
        "            break\n",
        "\n",
        "        if question.lower() == \"history\":\n",
        "            safe_print(\"\\n\" + assistant.get_conversation_summary() + \"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "            continue\n",
        "\n",
        "        if question.lower() == \"stats\":\n",
        "            safe_print(\"\\n\" + assistant.get_system_stats() + \"\\n\" + \"-\" * 60 + \"\\n\")\n",
        "            continue\n",
        "\n",
        "        if question.lower() == \"export\":\n",
        "            filepath = assistant.export_conversation()\n",
        "            if filepath:\n",
        "                safe_print(f\"‚úÖ Exported: {filepath}\\n\")\n",
        "            safe_print(\"-\" * 60 + \"\\n\")\n",
        "            continue\n",
        "\n",
        "        if question.lower() == \"clear\":\n",
        "            assistant.clear_history()\n",
        "            safe_print(\"-\" * 60 + \"\\n\")\n",
        "            continue\n",
        "\n",
        "        if question.lower() == \"reset\":\n",
        "            assistant.reset_system()\n",
        "            safe_print(\"-\" * 60 + \"\\n\")\n",
        "            continue\n",
        "\n",
        "        safe_print(\"\\nü§î Processing...\\n\")\n",
        "        answer = assistant.query(question)\n",
        "        safe_print(f\"ü§ñ Assistant:\\n{answer}\\n\" + \"-\" * 60 + \"\\n\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        safe_print(f\"\\n\\nüëã Interrupted. Data saved: {STORAGE_DIR}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        safe_print(f\"\\n‚ùå Error: {str(e)}\\n\")"
      ],
      "metadata": {
        "id": "BuMUcpqpcQwk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}